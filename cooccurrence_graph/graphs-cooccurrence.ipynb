{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurrence graphs\n",
    "This notebook describes how to create a graph of co-occurrences. For a full description, read [this](https://arxiv.org/pdf/2010.06710.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(*)\n",
    "\n",
    "#comment this line after you have downloaded the needed modules\n",
    "#specific things we need if you don't want to download everything:\n",
    "#english stopwords\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define some basic functions\n",
    "#that are going to be useful\n",
    "\n",
    "stop_list = stopwords.words('english')\n",
    "\n",
    "pos_to_wordnet = {\n",
    "    \"J\" : wordnet.ADJ,\n",
    "    \"V\" : wordnet.VERB,\n",
    "    \"R\" : wordnet.ADV\n",
    "}\n",
    "\n",
    "def tokenize(string_text):\n",
    "    return word_tokenize(string_text)\n",
    "\n",
    "def remove_stopwords(tokenized_text):\n",
    "    return [word for word in tokenized_text if word not in stop_list]\n",
    "\n",
    "def wordnet_pos(tokenized_text):\n",
    "    tagged_words = nltk.pos_tag(tokenized_text)\n",
    "    wordnet_words_tag = []\n",
    "    for w, tag in tagged_words:\n",
    "        first_letter = tag[0]\n",
    "        if first_letter in pos_to_wordnet:\n",
    "            wordnet_words_tag.append(pos_to_wordnet[first_letter])\n",
    "        else:\n",
    "            wordnet_words_tag.append(wordnet.NOUN)\n",
    "    return wordnet_words_tag\n",
    "\n",
    "def lemmatizer(tokenized_text_no_sw, pos_list):\n",
    "    return [WordNetLemmatizer().lemmatize(word, tag) for word, tag in zip(tokenized_text_no_sw, pos_list)]\n",
    "\n",
    "def preprocess_text(string_text):\n",
    "    tokenized_text = tokenize(string_text.lower())\n",
    "    #tokenized_text_no_sw = remove_stopwords(tokenized_text)\n",
    "    wordnet_pos_list = wordnet_pos(tokenized_text)\n",
    "    lemmatized = lemmatizer(tokenized_text, wordnet_pos_list)\n",
    "    return lemmatized, wordnet_pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNetwork(object):\n",
    "    def __init__(self, text, window):\n",
    "        padding = [\"<PAD>\"]*window\n",
    "        \n",
    "        self.text = padding + text + padding\n",
    "        self.window = window\n",
    "        self.vocabulary = set(self.text)\n",
    "        self.vocabulary_index = {x:i for i, x in enumerate(self.vocabulary)}\n",
    "        self.G, self.vocabulary_index = self.get_network()\n",
    "    \n",
    "    def get_network(self):\n",
    "        matrix = np.zeros((len(self.vocabulary), len(self.vocabulary)))\n",
    "        min_l = self.window \n",
    "        max_l = len(self.text) - self.window\n",
    "        for index, word in enumerate(self.text):\n",
    "            if index >= min_l and index < max_l:\n",
    "                index_central_word = self.vocabulary_index[word]\n",
    "                left_neighbors = self.text[index-self.window:index]\n",
    "                right_neighbors = self.text[index+1:index+self.window]\n",
    "                neighbors = set(left_neighbors + right_neighbors)\n",
    "                \n",
    "                for neighbor in neighbors:\n",
    "                    neighbor_index = self.vocabulary_index[neighbor]\n",
    "                    matrix[index_central_word, neighbor_index] = 1\n",
    "        G = nx.from_numpy_matrix(matrix)\n",
    "        return G, self.vocabulary_index\n",
    "        \n",
    "    def words_to_index(self, word):\n",
    "        index = self.vocabulary_index[word]\n",
    "        return index\n",
    "    \n",
    "    def get_degree(self, word):\n",
    "        index = self.words_to_index(word)\n",
    "        degree = self.G.degree(index)\n",
    "        return degree\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "t, pos = preprocess_text(\"this is a test to create a cooccurrences graph\")\n",
    "n = CNetwork(t,2).get_degree('graph')\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
