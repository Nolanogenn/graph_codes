{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95f390be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43973bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#layers = [-4, -3, -2, -1]\n",
    "layers = [-1]\n",
    "model = AutoModel.from_pretrained('bert-base-cased', output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df90fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_states(encoded, model, layers):\n",
    "    with torch.no_grad():\n",
    "         output = model(**encoded)\n",
    "    # Get all hidden states\n",
    "    states = output.hidden_states\n",
    "    # Stack and sum all requested layers\n",
    "    output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_words_vector(sent, tokenizer, model, layers):\n",
    "    encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\")\n",
    "    # get all token idxs that belong to the word of interest\n",
    "    #token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n",
    "\n",
    "    return get_hidden_states(encoded, model, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a859f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(\"dev_clean_format.txt\").readlines()\n",
    "corpus = [t.replace('\\n', '') for t in corpus]\n",
    "\n",
    "relations = pd.read_csv(\"dev_relations.tsv\", delimiter='\\t', header=None)\n",
    "relations.fillna('<NONE>')\n",
    "\n",
    "entities = pd.read_csv('dev_entities.tsv', delimiter='\\t', header=0)\n",
    "column_sentence = entities.columns[0]\n",
    "column_surface_form = entities.columns[1]\n",
    "column_pos = entities.columns[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab8f66b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1713 / 1714\r"
     ]
    }
   ],
   "source": [
    "list_of_networks = []\n",
    "\n",
    "for enum, sentence in enumerate(corpus):\n",
    "    print(enum, \"/\", len(corpus), end='\\r')\n",
    "    try:\n",
    "        network = nx.Graph()\n",
    "        edge_list = []\n",
    "        id_sentence = enum\n",
    "\n",
    "        specific_rel = relations.iloc[enum]    \n",
    "\n",
    "        doc = nlp(sentence)\n",
    "        tokens = [t for t in doc]\n",
    "\n",
    "        sent_embeddings = get_words_vector(sentence, tokenizer, model, layers)\n",
    "\n",
    "        id_token = 0\n",
    "        for enum, t in enumerate(tokens):\n",
    "            tokens_bert = tokenizer.tokenize(t.text, add_special_tokens=False)\n",
    "            token_idx = tokenizer.encode(t.text, add_special_tokens=False)\n",
    "            token_embeddings = []\n",
    "            for token_id in token_idx:\n",
    "                token_embeddings.append(sent_embeddings[id_token])\n",
    "                id_token += 1\n",
    "\n",
    "            if len(token_embeddings) > 1:\n",
    "                token_embeddings = torch.stack(token_embeddings)\n",
    "            else:\n",
    "                token_embeddings = token_embeddings[0]\n",
    "\n",
    "            edge = (t.i, t.head.i, t.dep_)\n",
    "            edge_list.append(edge)\n",
    "            network.add_node(t.i, embedding=token_embeddings)\n",
    "\n",
    "        for edge in edge_list:\n",
    "            network.add_edge(edge[0], edge[1], label=edge[2])\n",
    "\n",
    "        rel_label = specific_rel[0]\n",
    "        rel_subj = [i-1 for i in ast.literal_eval(specific_rel[1])]\n",
    "        rel_obj = [i-1 for i in ast.literal_eval(specific_rel[2])]\n",
    "\n",
    "        nodesubj = enum+1\n",
    "        nodeobj = enum+2\n",
    "\n",
    "        embeddings_subj = []\n",
    "        embeddings_obj = []\n",
    "\n",
    "        for n in range(rel_subj[0], rel_subj[1]+1):\n",
    "            #test this vs random initialization?\n",
    "            embeddings_subj.append(network.nodes[n]['embedding'])\n",
    "            network.add_edge(n, nodesubj, label=\"in_entity\")\n",
    "\n",
    "        for n in range(rel_obj[0], rel_obj[1]+1):\n",
    "            embeddings_obj.append(network.nodes[n]['embedding'])\n",
    "            network.add_edge(n, nodeobj, label=\"in_entity\")\n",
    "\n",
    "    \n",
    "        embeddings_subj = torch.stack(embeddings_subj)\n",
    "        embeddings_obj = torch.stack(embeddings_obj)\n",
    "\n",
    "        network.nodes[nodesubj]['embedding'] = embeddings_subj\n",
    "        network.nodes[nodeobj]['embedding'] = embeddings_obj\n",
    "\n",
    "        network.add_edge(nodesubj, nodeobj, label=rel_label)\n",
    "\n",
    "        for node in network.nodes():\n",
    "            network.nodes[node]['embedding'] = network.nodes[node]['embedding']\n",
    "\n",
    "        list_of_networks.append(network)\n",
    "        \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cf45014",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(network, 'dev_test_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5468c967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
